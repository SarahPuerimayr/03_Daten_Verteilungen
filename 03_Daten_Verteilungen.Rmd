---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(pacman)

```


# 1 Bernulli Verteilung

```{r}
set.seed(2010837870) #Random numbers generator is set to this position, so that random numbers can be reproduced later
#seed is set for this Chunk only!

# Generate bernoulli data # bernoulli distribution in r 
data_binom <- rbinom(n=100, size=1, prob=0.5)

ggplot(data.frame(data_binom),aes(data_binom))+
  geom_bar()+
  ggtitle("Bernulli Distribution with 100 trys")

# Plot the distribution # is it Fair/balanced? as expected?
```


# 2 Binomial Verteilung

```{r}
# Generate binomial data
data.binom2 <- rbinom(n=1000, size=1, p=0.8) 

# Plot the distribution
ggplot(tibble(data.binom2), aes(data.binom2))+
  geom_bar() #geom_histogram(bins=3)+
  ggtitle("Binomal Distribution")

# Assign and print probability of 8 or less successes
prob1 <- pbinom(8,10,0.8)

# Assign and print probability of all 10 successes
prob2 <- dbinom(10,10,0.8)

```


# 3 Normalverteilung

```{r}
# Generate normal data
mean
data_norm <- rnorm(1000, mean=2, sd=10)

# Plot distribution
ggplot(data=tibble(data_norm), aes(data_norm))+
  geom_histogram(bins = 30)+
  ggtitle("Normal distribution")

# Compute and print true probability for greater than 2
true_prob <- 1 - pnorm(q=2, sd=1)

# Compute and print sample probability for greater than 2 (by hand) - empirical for the sample
sample_prob <-  NULL #ToDo

```


#Aufgabe 4: Konfidenzintervall von Hand

```{r}
library(boot)
library(plotrix)
z_score <- 2.7764451051977987
nums <- c(1, 2, 3, 4, 5)
confidence <- 0.95

# Berechnen Sie den Standardfehler und die Fehlerquote.
std_err <- std.error(nums) # = sd(nums)/sqrt(length(nums))
margin_error <- pnorm(0.975)*std_err  

# Berechnen und ausgeben des unteren Schwellenwerts
lower <- z_score - margin_error 
lower

# Berechnen und ausgeben des oberen Schwellenwerts
upper <- z_score + margin_error
upper

```


# 5 Anwenden von Konfidenzintervallen

```{r}

heads <- sum(rbinom(50,1,0.5))
print(heads)

confidence_int <- prop.test(heads, 50, p=0.5, conf.level = 0.99) # 99% conf
confidence_int.9 <- prop.test(heads, 50, p=0.5, conf.level = 0.9)  # 90% Conf
confidence_int
confidence_int.9
# repeat 10 times

for(i in 1:10) {
  heads <- (sum(rbinom(50,1,0.5)))
  print(prop.test(heads, 50, p=0.5, conf.level = 0.9)$conf.int)
}
  
# check results
```



# 6 Samples eines Würfelwurfs

```{r}
# Create a sample of 10 dice rolls
small <- sample(6,10,replace=TRUE)

# Calculate and print the mean of the sample
small_mean <- mean(small)
small_mean

# Create a sample of 1000 die rolls
large <-  sample(6,1000,replace=TRUE)

# Calculate and print the mean of the large sample
large_mean  <-  mean(large)
large_mean

#Central Limit Theorem
#This phenomenon is termed the central limit theorem. It says that the means
#drawn from multiple samples will resemble the familiar bell-shaped normal
#curve (see “Normal Distribution”), even if the source population is not normally
#distributed, provided that the sample size is large enough and the departure of
#the data from normality is not too great
#Source: Bruce
```
```{python}
import numpy as np
array = np.array()

```



# 7 Simulation des Central Limit Theorem

```{r}

# Create a vector of 1000 sample means of size 30
samples <- sample(1:6,30,replace=TRUE)
stat_fun <- function(x, idx) mean(x[idx])
means <- boot(samples, R = 1000, stat_fun)

# Create and show a histogram of the means
ggplot(data.frame(means$t), aes(means$t))+
  geom_histogram() +
  ggtitle("Means of 1000 samples with size 30")

# Adapt code for 100 samples of size 30
means_100 <-  boot(samples, R = 1000, stat_fun)

# Create and show a histogram of the means
ggplot(data.frame(means_100$t), aes(means_100$t))+
  geom_histogram() +
  ggtitle("Means of 100 samples with size 30")

#Source: Foliensatz ILV (oder Video)
```




